"""
Legacy module that re-exports functionality from other modules.
This module is kept for backward compatibility but will be removed in a future version.
Please use the new modules directly:
- ai_processor for core AI functionality
- text_utils for text processing utilities
- vector_math for mathematical operations
- models for data classes
"""
import logging
from typing import Dict, Any, Optional

from backend.ai_processor import (
    analyze_text as detect_bias,
    rewrite_text as rewrite_for_balance,
    analyze_and_rewrite
)
from backend.text_utils import (
    clean_text,
    extract_main_content,
    compare_texts
)
from backend.models import BiasMetrics, SourceCredibility

logger = logging.getLogger(__name__)

from nltk.tokenize import sent_tokenize
from backend.ai_processor import detect_bias, rewrite_for_balance, detect_missing_context

# Setup logging
logging.basicConfig(level=logging.DEBUG)

def clean_text(text):
    """
    Clean article text by removing extra whitespace and normalizing text
    """
    # Remove extra spaces, newlines, and tabs
    text = re.sub(r'\s+', ' ', text)
    # Remove HTML tags if any
    text = re.sub(r'<[^>]+>', '', text)
    return text.strip()

def extract_main_content(text):
    """
    Extract the main content from article text, removing ads, navigation, etc.
    """
    # Basic implementation - more sophisticated content extraction would be needed
    # for production but this gives a starting point
    paragraphs = [p.strip() for p in text.split('\n\n') if p.strip()]
    # Filter out likely non-content paragraphs (too short, or containing common ad phrases)
    content_paragraphs = [p for p in paragraphs if len(p) > 50 and not re.search(r'(advertisement|subscribe|sign up)', p, re.IGNORECASE)]
    
    return '\n\n'.join(content_paragraphs)

def split_into_sections(text, max_length=1000):
    """
    Split large articles into manageable sections for analysis
    """
    try:
        # Try to use NLTK for sentence tokenization
        sentences = sent_tokenize(text)
    except Exception as e:
        logging.warning(f"NLTK sentence tokenization failed in split_into_sections: {str(e)}. Using fallback method.")
        # Fallback to enhanced regex-based method
        def fallback_sentence_tokenize(text):
            if not text:
                return []
                
            # First, normalize line breaks
            text = re.sub(r'\r\n|\r', '\n', text)
            
            # Handle common abbreviations to avoid splitting them (Mr., Dr., etc.)
            text = re.sub(r'(Mr\.|Mrs\.|Ms\.|Dr\.|Prof\.|St\.|etc\.|i\.e\.|e\.g\.)', lambda m: m.group().replace('.', '<POINT>'), text)
            
            # Try multiple matching patterns for robustness
            try:
                # Approach 1: Match standard sentence endings
                sentences = re.split(r'(?<=[.!?])\s+', text)
                if len(sentences) <= 1 and len(text) > 100:
                    # If first approach didn't work well, try another
                    sentences = re.split(r'(?<=[.!?:])\s+|(?<=\n)\s*', text)
            except Exception as regex_error:
                logging.warning(f"Regex-based sentence split failed: {regex_error}")
                # Emergency fallback: split by paragraphs
                sentences = [p.strip() for p in text.split('\n\n') if p.strip()]
                if not sentences:
                    # Last resort: return the whole text
                    sentences = [text]
            
            # Restore abbreviation periods and clean up sentences
            sentences = [s.replace('<POINT>', '.').strip() for s in sentences if s.strip()]
            
            return sentences
            
        sentences = fallback_sentence_tokenize(text)
    
    sections = []
    current_section = []
    current_length = 0
    
    for sentence in sentences:
        if current_length + len(sentence) > max_length and current_section:
            sections.append(' '.join(current_section))
            current_section = [sentence]
            current_length = len(sentence)
        else:
            current_section.append(sentence)
            current_length += len(sentence)
    
    if current_section:
        sections.append(' '.join(current_section))
    
    # If sections couldn't be created, split text by character length as last resort
    if not sections and text:
        logging.warning("Sentence tokenization failed. Splitting by character length.")
        text_length = len(text)
        for i in range(0, text_length, max_length):
            end = min(i + max_length, text_length)
            sections.append(text[i:end])
    
    return sections

class BiasMetrics:
    def __init__(self):
        self.sentiment_score = 0.0
        self.subjectivity_score = 0.0
        self.bias_categories = defaultdict(float)
        self.source_credibility = 0.0
        self.context_completeness = 0.0
        
    def to_dict(self) -> dict:
        return {
            'sentiment': round(self.sentiment_score, 2),
            'subjectivity': round(self.subjectivity_score, 2),
            'bias_categories': dict(self.bias_categories),
            'source_credibility': round(self.source_credibility, 2),
            'context_completeness': round(self.context_completeness, 2)
        }

class SourceCredibility:
    def __init__(self):
        self.credibility_cache = {}
        self.cache_file = 'credibility_cache.json'
        self._load_cache()
    
    def _load_cache(self):
        try:
            with open(self.cache_file, 'r') as f:
                self.credibility_cache = json.load(f)
        except (FileNotFoundError, json.JSONDecodeError):
            self.credibility_cache = {}
    
    def _save_cache(self):
        with open(self.cache_file, 'w') as f:
            json.dump(self.credibility_cache, f)
    
    def get_source_credibility(self, url: str) -> float:
        domain = urlparse(url).netloc
        
        if domain in self.credibility_cache:
            return self.credibility_cache[domain]
        
        # Calculate credibility score based on multiple factors
        score = self._analyze_source(domain)
        self.credibility_cache[domain] = score
        self._save_cache()
        
        return score
    
    def _analyze_source(self, domain: str) -> float:
        # Implement source analysis logic here
        # This would typically involve checking against known credible sources,
        # fact-checking organizations, and other metadata
        score = 0.7  # Default score
        return score

def analyze_sentiment(text: str) -> Tuple[float, float]:
    """
    Analyze the sentiment and subjectivity of the text using TextBlob
    Returns: (sentiment_score, subjectivity_score)
    """
    try:
        blob = textblob.TextBlob(text)
        return blob.sentiment.polarity, blob.sentiment.subjectivity
    except Exception as e:
        logging.error(f"Error in sentiment analysis: {e}")
        return 0.0, 0.0

def analyze_historical_context(text: str, topic: str) -> Dict[str, str]:
    """
    Analyze if important historical context is missing from the article
    """
    # This would typically involve checking against a knowledge base
    # For now, we'll use a simplified implementation
    context_results = {
        'missing_context': [],
        'related_events': [],
        'time_period': detect_time_period(text)
    }
    return context_results

def detect_time_period(text: str) -> str:
    """
    Detect the time period discussed in the text
    """
    # Implementation would involve NLP-based temporal expression extraction
    # For now, return a placeholder
    return "contemporary"

def find_similar_articles(text: str, url: str = None) -> List[dict]:
    """
    Find similar articles for comparative analysis
    """
    # This would typically involve:
    # 1. Text embedding
    # 2. Similarity search in a database
    # 3. Retrieving and comparing articles
    # For now, return a simplified implementation
    return []

def analyze_article(text: str, url: str = None) -> Dict:
    """
    Enhanced article analysis with advanced bias detection
    """
    text = clean_text(text)
    content = extract_main_content(text)
    
    # Initialize metrics
    metrics = BiasMetrics()
    
    # Sentiment and subjectivity analysis
    sentiment, subjectivity = analyze_sentiment(content)
    metrics.sentiment_score = sentiment
    metrics.subjectivity_score = subjectivity
    
    # Source credibility
    if url:
        source_checker = SourceCredibility()
        metrics.source_credibility = source_checker.get_source_credibility(url)
    
    # Bias detection
    bias_results = detect_bias(content)
    metrics.bias_categories.update(bias_results['categories'])
    
    # Historical context analysis
    context_results = analyze_historical_context(content, bias_results.get('topic', ''))
    metrics.context_completeness = len(context_results['missing_context']) * -0.1 + 1.0
    
    # Find similar articles for comparison
    similar_articles = find_similar_articles(content, url)
    
    # Generate balanced rewrite suggestions
    rewrite_suggestions = rewrite_for_balance(content, bias_results)
    
    return {
        'metrics': metrics.to_dict(),
        'bias_detected': bias_results['bias_detected'],
        'categories': bias_results['categories'],
        'context': context_results,
        'similar_articles': similar_articles,
        'suggestions': rewrite_suggestions,
        'original_text': content
    }

def compare_texts(original, rewritten):
    """
    Generate a comparison between original and rewritten text
    
    Args:
        original (str): The original text
        rewritten (str): The rewritten text
        
    Returns:
        dict: Comparison information including changes and diff
    """
    try:
        # Split texts into sentences for comparison
        try:
            # Try to use NLTK tokenizer
            original_sentences = sent_tokenize(original)
            rewritten_sentences = sent_tokenize(rewritten)
        except Exception as e:
            logging.warning(f"NLTK sentence tokenization failed: {str(e)}. Using fallback method.")
            # Fallback to a more sophisticated regex-based method that handles common sentence ending patterns
            def fallback_sentence_tokenize(text):
                if not text:
                    return []
                    
                # First, normalize line breaks
                text = re.sub(r'\r\n|\r', '\n', text)
                
                # Handle common abbreviations to avoid splitting them (Mr., Dr., etc.)
                text = re.sub(r'(Mr\.|Mrs\.|Ms\.|Dr\.|Prof\.|St\.|etc\.|i\.e\.|e\.g\.)', lambda m: m.group().replace('.', '<POINT>'), text)
                
                # Try multiple matching patterns for robustness
                try:
                    # Approach 1: Match standard sentence endings
                    sentences = re.split(r'(?<=[.!?])\s+', text)
                    if len(sentences) <= 1 and len(text) > 100:
                        # If first approach didn't work well, try another
                        sentences = re.split(r'(?<=[.!?:])\s+|(?<=\n)\s*', text)
                except Exception as regex_error:
                    logging.warning(f"Regex-based sentence split failed: {regex_error}")
                    # Emergency fallback: split by paragraphs
                    sentences = [p.strip() for p in text.split('\n\n') if p.strip()]
                    if not sentences:
                        # Last resort: return the whole text
                        sentences = [text]
                
                # Restore abbreviation periods and clean up sentences
                sentences = [s.replace('<POINT>', '.').strip() for s in sentences if s.strip()]
                
                return sentences
                
            original_sentences = fallback_sentence_tokenize(original)
            rewritten_sentences = fallback_sentence_tokenize(rewritten)
        
        # Use difflib to get differences
        differ = difflib.Differ()
        diff = list(differ.compare(original_sentences, rewritten_sentences))
        
        # Calculate statistics
        added = len([line for line in diff if line.startswith('+ ')])
        removed = len([line for line in diff if line.startswith('- ')])
        changed = len([line for line in diff if line.startswith('? ')])
        
        # Format the differences for display
        formatted_diff = []
        for i, line in enumerate(diff):
            if line.startswith('  '):  # unchanged
                formatted_diff.append({"type": "unchanged", "text": line[2:]})
            elif line.startswith('- '):  # removed
                formatted_diff.append({"type": "removed", "text": line[2:]})
            elif line.startswith('+ '):  # added
                formatted_diff.append({"type": "added", "text": line[2:]})
        
        return {
            "changes": {
                "added": added,
                "removed": removed,
                "changed": changed
            },
            "diff": formatted_diff
        }
    except Exception as e:
        logging.error(f"Error comparing texts: {str(e)}")
        # Return a simple diff if all else fails
        return {
            "changes": {
                "added": 0,
                "removed": 0,
                "changed": 0
            },
            "diff": [
                {"type": "unchanged", "text": original},
                {"type": "added", "text": rewritten}
            ]
        }
